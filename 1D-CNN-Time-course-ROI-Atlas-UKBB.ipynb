{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clasification based on function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of ASD vs Controls based on different atlases.\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os.path as osp\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunch wandb ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/zmohaghegh/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 390734ff44d817dbba59927d4eb542e564627b3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abide1DConvNet(nn.Module):\n",
    "    def __init__(self, nROIS):\n",
    "        super(Abide1DConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels= nROIS, out_channels=64, kernel_size=3)\n",
    "        self.avg = nn.AdaptiveAvgPool1d((1))\n",
    "        self.linear1 = nn.Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.avg(x).view(-1, 64)\n",
    "        x = self.linear1(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        \n",
    "        #print(x.shape, x.min(), x.max())\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(net, val_data_loader, fold):\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    net.eval()\n",
    "    \n",
    "    total=0\n",
    "    correct=0\n",
    "    total_valid_loss=0\n",
    "    \n",
    "    for i, data in enumerate(val_data_loader,0):\n",
    "\n",
    "        inputs, labels= data\n",
    "        labels = labels.double()\n",
    "   \n",
    "        # forward pass\n",
    "        output = net(inputs)\n",
    "        outputs=output.squeeze(1)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        predict = outputs.data > 0.0\n",
    "        \n",
    "        total_valid_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "        \n",
    "    wandb.log({f\"valid_Loss_fold_{fold}\":total_valid_loss/total, f\"valid_acc_fold_{fold}\": 100 * correct / total })\n",
    "\n",
    "    \n",
    "    # Calculate acc\n",
    "    valid_accuracy= 100 * correct /total\n",
    "    valid_loss = total_valid_loss/total\n",
    "\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, nepochs, batch_size, learning_rate, fold, atlas_name, nr_RIO):\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    net = Abide1DConvNet(nROIS=nr_RIO)\n",
    "    net = net.double()\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss() # weight=class_weigths\n",
    "    optimizer = optim.SGD(net.parameters(), momentum=0.9, lr = learning_rate, weight_decay=0.02)\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    best_val_acc = None\n",
    "    #best_net_valid = None\n",
    "\n",
    "    for i_epoch in range(nepochs):\n",
    "        \n",
    "        total=0\n",
    "        correct=0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_data_loader, 0):\n",
    "            \n",
    "            inputs, labels = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = net(inputs)\n",
    "            outputs=output.squeeze(1)\n",
    "            \n",
    "            labels=labels.double()\n",
    "            \n",
    "            # prediction \n",
    "            predicted = outputs.data > 0  # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "            \n",
    "            # print(outputs,predicted)\n",
    "            \n",
    "            # Calculate loss\n",
    "            #print(outputs.size() , labels.size())\n",
    "            \n",
    "            loss = criterion(outputs, labels)# [32,1] ---> [32]\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            #print(correct,total,loss)\n",
    "        \n",
    "        wandb.log({\"epoch\": i_epoch,\"train_Loss\":train_loss/total, \"train_acc\": 100 * correct / total })\n",
    "\n",
    "        # Validation\n",
    "        epoch_val_loss, epoch_val_acc = validate_model(net, val_data_loader, fold)\n",
    "        \n",
    "        #Save model with best validation loss\n",
    "        if not best_val_acc or best_val_acc < epoch_val_acc:\n",
    "            best_net_valid = net\n",
    "            best_val_acc = epoch_val_acc\n",
    "            \n",
    "            #path_best_loss_atlas = f'/data/zmohaghegh/TempStats_3D-CNN/best_model_atlas/best_model_atlast.pth'\n",
    "            #torch.save(net.state_dict(), path_best_loss_atlas)\n",
    "                                    \n",
    "    return best_net_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, test_data, batch_size):\n",
    "    \n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    total = 0\n",
    "    correct=0\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    \n",
    "    for i, data in enumerate(test_data_loader,0):\n",
    "        inputs, labels = data\n",
    "            \n",
    "        # forward pass\n",
    "        output = net(inputs)\n",
    "        outputs=output.squeeze(1)\n",
    "        labels=labels.double()\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        predict = outputs.data > 0.0\n",
    "            \n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "    \n",
    "    # Calculate acc\n",
    "    test_acc= 100 * correct /total\n",
    "    wandb.log({\"test_acc\": test_acc })\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(atlas_name,nTime_min, zscore,folds,nepochs,batch_size,learning_rate,\n",
    "              root_dir='/dbstore/zmohaghegh/UKBiobank_subset/Time_course_New/', \n",
    "              data_info_file='data_info.csv',\n",
    "              exp_dir='/data/zmohaghegh/TempStats_3D-CNN/atlas_model/'):    \n",
    "    \n",
    "        print(\"preparing dataset ....\")\n",
    "        \n",
    "        # Read the parent CSV file\n",
    "        data_info = pd.read_csv(os.path.join(root_dir, data_info_file))\n",
    "        data_info = shuffle(data_info)\n",
    "        \n",
    "        # Determine the nchannels (=nrois) from the data by using the first sample\n",
    "        sample_file = data_info['tc_file'].iloc[0].replace('ATLAS', atlas_name)\n",
    "        nrois = pd.read_csv(sample_file).values.shape[1] # number of channel = number of brain region in Atlas\n",
    "        \n",
    "        total_subjects = len(data_info)\n",
    "        \n",
    "        # Initialize an np array to store all timecourses and labels\n",
    "        tc_data = np.zeros((total_subjects, nrois, nTime_min))\n",
    "        labels = np.zeros(total_subjects, dtype=int)\n",
    "        \n",
    "        # Load data       \n",
    "        for i, sub_i in enumerate(data_info.index):\n",
    "            tc_file = data_info['tc_file'].loc[sub_i].replace('ATLAS', atlas_name)\n",
    "            tc_vals = pd.read_csv(tc_file).values.transpose()[:, :nTime_min]\n",
    "\n",
    "            if zscore:       \n",
    "                tc_vals =  np.array([(tc_vals[:,i] - np.mean(tc_vals[:,i]))/np.std(tc_vals[:,i]) for i in range (tc_vals.shape[1])])\n",
    "                tc_data[i] = tc_vals.transpose()\n",
    "            else:\n",
    "                tc_data[i] = tc_vals ### might need \"transponse\"\n",
    "\n",
    "            labels[i] = data_info['DX_GROUP'].loc[sub_i]\n",
    "            \n",
    "        #labels = np.eye(2)[labels] \n",
    "        \n",
    "        kfold = KFold(n_splits=folds, shuffle=True)\n",
    "        #kfold = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "        total_accuracy=0\n",
    "        total_sensitivity=0\n",
    "        total_specificity=0\n",
    "        \n",
    "        # loop  cross validation over folds\n",
    "        \n",
    "        for fold, (train_index, test_index) in enumerate(kfold.split(tc_data)):\n",
    "        #for fold, (train_index, test_index) in enumerate(kfold.split(tc_data, labels)):\n",
    "            print(f'Fold_{fold}_Atlas_{atlas_name}------------')\n",
    "            \n",
    "            #spltitting training fold into 90% training and 10% validation    \n",
    "            train_split = int(0.9 * len(train_index))\n",
    "            train_i = train_index[0:train_split]\n",
    "            val_i = train_index[train_split:]\n",
    "            \n",
    "            # nested Stratified kfold\n",
    "            # train_val_folds = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "            # train_i, val_i = list(train_val_folds.split(tc_data[train_index], labels[train_index]))[0]\n",
    "            \n",
    "            # nested KFOLD\n",
    "            #train_val_folds = KFold(n_splits=2, shuffle=True)\n",
    "            #train_i, val_i = list(train_val_folds.split(tc_data[train_index], labels[train_index]))[0]\n",
    "            \n",
    "            # Create training,testing and validation datasets\n",
    "            train_data = torch.from_numpy(tc_data[train_i])\n",
    "            train_labels= torch.from_numpy(labels[train_i])\n",
    "            \n",
    "            val_data = torch.from_numpy(tc_data[val_i])\n",
    "            val_labels = torch.from_numpy(labels[val_i])\n",
    "            \n",
    "            test_data = torch.from_numpy(tc_data[test_index])\n",
    "            test_labels = torch.from_numpy(labels[test_index])\n",
    "                   \n",
    "            train = data_utils.TensorDataset(train_data, train_labels)\n",
    "            val = data_utils.TensorDataset(val_data, val_labels)\n",
    "            test = data_utils.TensorDataset(test_data, test_labels)\n",
    "            \n",
    "            #train network\n",
    "            print('Start Training ...')\n",
    "\n",
    "            validated_network = train_model(train,val,nepochs,batch_size,learning_rate,fold,atlas_name,nrois)\n",
    "            print('Validation finished...')\n",
    "            \n",
    "            #test network\n",
    "            print('Start Testing...')\n",
    "\n",
    "            test_accuracy= test_model(validated_network,test,batch_size)\n",
    "            \n",
    "            total_accuracy += test_accuracy\n",
    "            #total_sensitivity += test_sens\n",
    "            #total_specificity += test_spec\n",
    "                        \n",
    "            print(\"----Test results of of fold {} are : {} acc ----\".format(fold, test_accuracy))\n",
    "                \n",
    "        acc_test_average = total_accuracy/folds\n",
    "        #sens_test_average = total_sensitivity/folds\n",
    "        #spec_test_average = total_specificity/folds\n",
    "                \n",
    "        return acc_test_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:god6yuhw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6764<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210613_144332-god6yuhw/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210613_144332-god6yuhw/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">eager-meadow-15</strong>: <a href=\"https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal/runs/god6yuhw\" target=\"_blank\">https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal/runs/god6yuhw</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:god6yuhw). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">eager-cherry-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal\" target=\"_blank\">https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal/runs/x5wn98nt\" target=\"_blank\">https://wandb.ai/zahramhn/1d-cnn-UKBB-timecourse-atlas-aal/runs/x5wn98nt</a><br/>\n",
       "                Run data is saved locally in <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210613_144402-x5wn98nt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing dataset ....\n",
      "Fold_0_Atlas_AAL------------\n",
      "Start Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation finished...\n",
      "Start Testing...\n",
      "----Test results of of fold 0 are : 57.926829268292686 acc ----\n",
      "Fold_1_Atlas_AAL------------\n",
      "Start Training ...\n",
      "Validation finished...\n",
      "Start Testing...\n",
      "----Test results of of fold 1 are : 56.707317073170735 acc ----\n",
      "Fold_2_Atlas_AAL------------\n",
      "Start Training ...\n",
      "Validation finished...\n",
      "Start Testing...\n",
      "----Test results of of fold 2 are : 59.146341463414636 acc ----\n",
      "Fold_3_Atlas_AAL------------\n",
      "Start Training ...\n",
      "Validation finished...\n",
      "Start Testing...\n",
      "----Test results of of fold 3 are : 53.987730061349694 acc ----\n",
      "Fold_4_Atlas_AAL------------\n",
      "Start Training ...\n",
      "Validation finished...\n",
      "Start Testing...\n",
      "----Test results of of fold 4 are : 55.214723926380366 acc ----\n",
      "******Accuracy_test_Avg********= 56.59658835852163\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ntimes  = [200]  #100,200,300,500\n",
    "atlases = ['AAL'] #'HO_cort_maxprob_thr25-2mm'\n",
    "\n",
    "# hyperparameter\n",
    "    \n",
    "batch_size = 1\n",
    "learning_rate =0.0001\n",
    "nepochs = 150\n",
    "zscore=True\n",
    "nr_folds = 5\n",
    "\n",
    "for atlas in atlases:\n",
    "    wandb.init(project=f'1d-cnn-UKBB-timecourse-atlas-{atlas}')\n",
    "    for ntime in ntimes:\n",
    "        accuracy_test_total = run_kfold(zscore=zscore,folds =nr_folds,atlas_name=atlas,nTime_min=ntime, \n",
    "                                        nepochs =nepochs, batch_size = batch_size ,learning_rate =learning_rate)\n",
    "        print(f'******Accuracy_test_Avg********= {accuracy_test_total}')\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntimes  = [200]\n",
    "atlases = ['HO_cort_maxprob_thr25-2mm']\n",
    "\n",
    "# hyperparameter\n",
    "    \n",
    "batch_size = 1\n",
    "learning_rate =0.0005\n",
    "nepochs = 250\n",
    "zscore=True\n",
    "nr_folds = 5\n",
    "\n",
    "for atlas in atlases:\n",
    "    wandb.init(project=f'1d-cnn-UKBB-timecourse-atlas-{atlas}')\n",
    "    for ntime in ntimes:\n",
    "        accuracy_test_total = run_kfold(zscore=zscore,folds =nr_folds,atlas_name=atlas,nTime_min=ntime, nepochs =nepochs, batch_size = batch_size ,learning_rate =learning_rate)\n",
    "        print(f'******Accuracy_test_Avg********= {accuracy_test_total}')\n",
    "        print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info == !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info = pd.read_csv('/dbstore/zmohaghegh/Japanese_subset/Time_course/data_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv('/dbstore/zmohaghegh/Japanese_subset/Time_course/data_info_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir='/dbstore/zmohaghegh/Japanese_subset/atlas_model/',\n",
    "atlas_name='AAL'\n",
    "root_dir='/dbstore/zmohaghegh/Japanese_subset/Time_course/', \n",
    "data_info_file='data_info.csv'\n",
    "\n",
    "nTime_min=84 \n",
    "zscore=False\n",
    "folds = 5    \n",
    "kfold = KFold(n_splits=folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyper_parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = pd.read_csv('/dbstore/zmohaghegh/Japanese_subset/Time_course/data_info.csv')\n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preparing dataset \")\n",
    "        \n",
    "# Read the parent CSV file\n",
    "data_info = pd.read_csv('/dbstore/zmohaghegh/Japanese_subset/Time_course/data_info.csv')\n",
    "data_info = shuffle(data_info,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTime_min = 84\n",
    "\n",
    "# Determine the nchannels (atlas region) (=nrois) from the data by using the first sample\n",
    "#sample_file = pd.read_csv('/dbstore/zmohaghegh/Japanese_subset/Time_course/sub-0487/tc/AALtimecourse.csv')\n",
    "sample_file = pd.read_csv(data_info['tc_file'].iloc[0].replace('ATLAS', atlas_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrois = sample_file.values.shape[1]\n",
    "total_subjects = len(data_info)\n",
    "nrois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an np array to store all timecourses and labels\n",
    "tc_data = np.zeros((total_subjects, nrois, nTime_min))\n",
    "labels = np.zeros(total_subjects, dtype=int)\n",
    "ids = np.zeros(total_subjects, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data       \n",
    "for i, sub_i in enumerate(data_info.index):\n",
    "    tc_file = data_info['tc_file'].loc[sub_i].replace('ATLAS', atlas_name)\n",
    "    tc_vals = pd.read_csv(tc_file).values.transpose()[:, :nTime_min]\n",
    "\n",
    "    if (zscore):       \n",
    "        tc_vals =  np.array([(tc_vals[:,i] - np.mean(tc_vals[:,i]))/np.std(tc_vals[:,i]) for i in range (tc_vals.shape[1])])\n",
    "        tc_data[i] = tc_vals.transpose()\n",
    "    else:\n",
    "        tc_data[i] = tc_vals\n",
    "\n",
    "    labels[i] = data_info['DX_GROUP'].loc[sub_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_info['tc_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.eye(2)[labels] \n",
    "kfold = KFold(folds, True, 1)\n",
    "\n",
    "j = 0\n",
    "total_accuracy=0\n",
    "accuracies=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login 390734ff44d817dbba59927d4eb542e564627b3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross validation \n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(tc_data)):\n",
    "                \n",
    "    # Spltitting training fold into 90% training and 10% validation    \n",
    "    train_split = int(0.8 * len(train_index))\n",
    "    train_i = train_index[0:train_split]\n",
    "    val_i = train_index[train_split:]\n",
    "\n",
    "            \n",
    "    # Create training,testing and validation datasets\n",
    "    train_data = torch.from_numpy(tc_data[train_i])\n",
    "    train_labels= torch.from_numpy(labels[train_i])\n",
    "    \n",
    "            \n",
    "    val_data = torch.from_numpy(tc_data[val_i])\n",
    "    val_labels = torch.from_numpy(labels[val_i])\n",
    "    #print(val_i,tc_data[val_i])\n",
    "    \n",
    "            \n",
    "    test_data = torch.from_numpy(tc_data[test_index])\n",
    "    test_labels = torch.from_numpy(labels[test_index])\n",
    "            \n",
    "    train = data_utils.TensorDataset(train_data, train_labels)\n",
    "    val = data_utils.TensorDataset(val_data, val_labels)\n",
    "    test = data_utils.TensorDataset(test_data, test_labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_data[test_index][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def train_model():\n",
    "#wandb.init(project='1D-CNN-atlast-timecourse')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "atlas_name='AAL'\n",
    "fold=4\n",
    "nr_RIO=nrois\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate =.0001\n",
    "nepochs = 20\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "net = Abide1DConvNet(nROIS=nr_RIO)\n",
    "net= net.double()\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss() # weight=class_weigths\n",
    "optimizer =  optim.Adam(net.parameters(), lr=learning_rate , weight_decay=0.02)\n",
    "#optimizer = optim.SGD(net.parameters(), momentum=0.9, lr = learning_rate, weight_decay=0.02)\n",
    "    \n",
    "net.train()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "    \n",
    "best_val_acc = None\n",
    "#best_net_valid = None\n",
    "\n",
    "for i_epoch in range(nepochs):\n",
    "        \n",
    "    total=0\n",
    "    correct=0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "        #inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = net(inputs)\n",
    "        outputs=output.squeeze(1)\n",
    "\n",
    "        labels=labels.double()\n",
    "        #print(i, labels,outputs)\n",
    "\n",
    "        # prediction \n",
    "        predicted = outputs.data >0  # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "\n",
    "        # print(outputs,predicted)\n",
    "\n",
    "        # Calculate loss\n",
    "        # print(outputs.size() , labels.size())\n",
    "\n",
    "        loss = criterion(outputs, labels)# [32,1] ---> [32]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        epoch_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        #print(total,correct)\n",
    "        \n",
    "    acc= 100 * correct / total\n",
    "    print(f'acc_{acc}')    \n",
    "    #wandb.log({\"epoch\": i_epoch , f\"train_Loss_fold_{fold}_Atlas_{atlas_name}\":epoch_loss/total, f\"train_acc_fold_{fold}_Atlas_{atlas_name}\": 100 * correct / total })\n",
    "\n",
    "    # Validation\n",
    "    epoch_val_loss, epoch_val_acc = validate_model(net, val_data_loader)\n",
    "    #print(f' vali acc: {epoch_val_acc}')\n",
    "    \n",
    "    #Save model with best validation loss\n",
    "    if not best_val_acc or best_val_acc < epoch_val_acc:\n",
    "        best_net_valid = net\n",
    "        best_val_acc = epoch_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        wandb.log({\"epoch\": i_epoch,f\"train_Loss_fold_{fold}_Atlas_{atlas_name}\":epoch_loss/total, f\"train_acc_fold_{fold}_Atlas_{atlas_name}\": 100 * correct / total })\n",
    "\n",
    "        # Validation\n",
    "        epoch_val_loss, epoch_val_acc = validate_model(net, val_data_loader)\n",
    "        print(epoch_val_acc, best_val_acc)\n",
    "        \n",
    "        #Save model with best validation loss\n",
    "        if not best_val_acc or best_val_acc < epoch_val_acc:\n",
    "            best_net_valid = net\n",
    "            best_val_acc = epoch_val_acc\n",
    "            \n",
    "            #path_best_loss_atlas = f'/data/zmohaghegh/TempStats_3D-CNN/best_model_atlas/best_model_atlast.pth'\n",
    "            #torch.save(net.state_dict(), path_best_loss_atlas)\n",
    "                                    \n",
    "    return best_net_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = total_accuracy/folds\n",
    "#sens = total_sensitivity/folds\n",
    "#spec = total_specificity/folds\n",
    "        \n",
    "print(\"{} in {} nTime results are: {} acc. \".format(atlas_name, nTime_min, acc))\n",
    "print(acc,total_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list_info = pd.read_csv('UKBB_merged_subjects_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese subset_ main thesis project -ReHo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score,confusion_matrix\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Convolutional Neural Network : BASED ON https://www.biorxiv.org/content/10.1101/2019.12.17.879346v1\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.downsample = nn.AvgPool3d(2, stride=2, padding=0)\n",
    "        \n",
    "        self.CNNlayer = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv3d(64, 16, kernel_size=3, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "        \n",
    "        self.flat1 = nn.Linear(160000, 16)   \n",
    "        self.flat2 = nn.Linear(16, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x=self.downsample(x)\n",
    "        #print(f'avg-pool: {x.size()}\\n----------')\n",
    "        #print(f'number of nan in this layer = {torch.isnan(x).sum()}')\n",
    "        \n",
    "        x=self.CNNlayer(x)\n",
    "        #print(f'convolution1+2+maxpool: {x.size()} \\n----------')\n",
    "        \n",
    "        x=x.reshape(x.size(0), -1)\n",
    "        #print(f'reshape after cnn: {x.size()}\\n----------')\n",
    "        \n",
    "        x=F.elu(self.flat1(x))\n",
    "        #print(f'fully-connected1: {x.size()}\\n----------')\n",
    "                    \n",
    "        x=self.flat2(x)\n",
    "        #print(f'fully-connected2: {x.size()}\\n----------')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lunch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/zmohaghegh/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 390734ff44d817dbba59927d4eb542e564627b3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funtion for preparaing summary measure for feeding to neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparaing_summary_measure(mdd_test_site_reho_load,control_test_site_reho_load):\n",
    "    \n",
    "    # load numpy \n",
    "    control_reho_zero_nan2= control_test_site_reho_load\n",
    "    mdd_reho_zero_nan2 = mdd_test_site_reho_load\n",
    "\n",
    "    # create empty numpy \n",
    "    control_reho_zero_nan3 =np.array(control_reho_zero_nan2)\n",
    "    mdd_reho_zero_nan3 = np.array(mdd_reho_zero_nan2)\n",
    "\n",
    "    # zero_nan_control\n",
    "    for i in range(len(control_test_site_reho_load)):    \n",
    "        control_reho_zero_nan3[i][0] =np.nan_to_num(control_test_site_reho_load[i][0],copy=True)\n",
    "        control_reho_zero_nan3[i][1] =np.nan_to_num(control_test_site_reho_load[i][1],copy=True)\n",
    "\n",
    "    # zero_nan_mdd\n",
    "    for i in range(len(mdd_test_site_reho_load)):    \n",
    "        mdd_reho_zero_nan3[i][0] =np.nan_to_num(mdd_test_site_reho_load[i][0],copy=True)\n",
    "        mdd_reho_zero_nan3[i][1] =np.nan_to_num(mdd_test_site_reho_load[i][1],copy=True)\n",
    "\n",
    "    # add one channel to zero nan\n",
    "    control_reho_4d_zero_nan = [[np.reshape(c[0], (1, 91, 109, 91)), c[1]] for c in control_reho_zero_nan3]\n",
    "    mdd_reho_4d_zero_nan = [[np.reshape(m[0], (1, 91, 109, 91)), m[1]] for m in mdd_reho_zero_nan3]\n",
    "\n",
    "    # concat mdd and contor data  \n",
    "    dataset_Japan_test_reho_zero_nan= ConcatDataset([control_reho_4d_zero_nan, mdd_reho_4d_zero_nan])\n",
    "    \n",
    "    #dataset_Japan_test_reho_zero_nan[10][0][0,:,54,45]\n",
    "    \n",
    "    return dataset_Japan_test_reho_zero_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Japanese IDs for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI_MDD=pd.read_csv('COI_MDD.csv')\n",
    "UTO_MDD=pd.read_csv('UTO_MDD.csv')\n",
    "HUH_MDD=pd.read_csv('HUH_MDD.csv')\n",
    "HKH_MDD=pd.read_csv('HKH_MDD.csv')\n",
    "HRC_MDD=pd.read_csv('HRC_MDD.csv')\n",
    "KUT_MDD=pd.read_csv('KUT_MDD.csv')\n",
    "\n",
    "MDD_sites=[COI_MDD,UTO_MDD,HUH_MDD,HKH_MDD,HRC_MDD,KUT_MDD]\n",
    "MDD_sites_concat=pd.concat([COI_MDD,UTO_MDD,HUH_MDD,HKH_MDD,HRC_MDD,KUT_MDD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COI_Control=pd.read_csv('COI_Control.csv')\n",
    "UTO_Control=pd.read_csv('UTO_Control.csv')\n",
    "HUH_Control=pd.read_csv('HUH_Control.csv')\n",
    "HKH_Control=pd.read_csv('HKH_Control.csv')\n",
    "HRC_Control=pd.read_csv('HRC_Control.csv')\n",
    "KUT_Control=pd.read_csv('KUT_Control.csv')\n",
    "\n",
    "Control_sites=[COI_Control,UTO_Control,HUH_Control,HKH_Control,HRC_Control,KUT_Control]\n",
    "Control_sites_concat=pd.concat([COI_Control,UTO_Control,HUH_Control,HKH_Control,HRC_Control,KUT_Control])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdd_base_path = '/dbstore/zmohaghegh/Japanese_subset/summary_measures/MDD_reho/ReHo_Normalised_z/'\n",
    "control_base_path = '/dbstore/zmohaghegh/Japanese_subset/summary_measures/Control_reho/ReHo_Normalised_z/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(mdd_test_site_reho_load)\n",
    "#len(mdd_test_site_reho_load[0])\n",
    "#mdd_test_site_reho_load[0][0].shape\n",
    "#test_dataset_cv =  preparaing_summary_measure(mdd_test_site_reho_load, mdd_test_site_reho_load) \n",
    "#print(len(test_dataset_cv))\n",
    "#test_dataset_cv[0][0].shape\n",
    "#len(MDD_sites_concat['participants_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Control dataset size : 62\n"
     ]
    }
   ],
   "source": [
    "Control_test= Control_sites[site_num] \n",
    "ID_test_Control=[]\n",
    "for j in range(len(Control_test['participants_id'])):\n",
    "    ID_test_Control.append(Control_test['participants_id'][j])\n",
    "print(f'test Control dataset size : {len(ID_test_Control)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Control dataset size : 189\n"
     ]
    }
   ],
   "source": [
    "ID_train_Control=[]\n",
    "for ids in Control_sites_concat['participants_id']:\n",
    "    if ids not in ID_test_Control:\n",
    "        ID_train_Control.append(ids)\n",
    "print(f'train Control dataset size : {len(ID_train_Control)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave on site out : Cross validation loop for each Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COI\n",
      "loading test dataset\n",
      "test MDD dataset size : 71\n",
      "concating train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataset\n",
      "train MDD dataset size : 184\n",
      "concating train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzahramhn\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">light-flower-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho\" target=\"_blank\">https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/ehm4b7jq\" target=\"_blank\">https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/ehm4b7jq</a><br/>\n",
       "                Run data is saved locally in <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210501_005346-ehm4b7jq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss= 0.006837654547152946\n",
      "train Acc= 100.0\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 5.179052929375922e-05\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 0: 100 %\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.005770599559262714\n",
      "train Acc= 96.59863945578232\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 5.152145186582089e-07\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 1: 100 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.005716263624398767\n",
      "train Acc= 96.59863945578232\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 5.615741433182553e-07\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 2: 100 %\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.009499513273826329\n",
      "train Acc= 97.96610169491525\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 6.692716083222428e-05\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 3: 100 %\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.0055140642634262105\n",
      "train Acc= 96.61016949152543\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 9.077033782756979e-07\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 4: 100 %\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR japanese reho 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 100.0 %\n",
      "Fold 1: 100.0 %\n",
      "Fold 2: 100.0 %\n",
      "Fold 3: 100.0 %\n",
      "Fold 4: 100.0 %\n",
      "Average: 100.0 %\n",
      "Start TEST FOR FOLD 0\n",
      "test_Acc_CV\": 100.0\n",
      "F1_score CV ReHo :1.0\n",
      "Balanced ACC CV ReHo :1.0\n",
      "Loss CV ReHo : 3.898800033809761e-05\n",
      "Start TEST FOR FOLD 1\n",
      "test_Acc_CV\": 100.0\n",
      "F1_score CV ReHo :1.0\n",
      "Balanced ACC CV ReHo :1.0\n",
      "Loss CV ReHo : 1.6940367826143558e-07\n",
      "Start TEST FOR FOLD 2\n",
      "test_Acc_CV\": 100.0\n",
      "F1_score CV ReHo :1.0\n",
      "Balanced ACC CV ReHo :1.0\n",
      "Loss CV ReHo : 2.1456727266182842e-07\n",
      "Start TEST FOR FOLD 3\n",
      "test_Acc_CV\": 100.0\n",
      "F1_score CV ReHo :1.0\n",
      "Balanced ACC CV ReHo :1.0\n",
      "Loss CV ReHo : 3.349344166805103e-05\n",
      "Start TEST FOR FOLD 4\n",
      "test_Acc_CV\": 100.0\n",
      "F1_score CV ReHo :1.0\n",
      "Balanced ACC CV ReHo :1.0\n",
      "Loss CV ReHo : 2.9641675835570635e-07\n",
      " Average Balance ACC japan ReHo = 100.0\n",
      "standard deviation :0.0\n",
      " Average F1_score japan ReHo = 1.0\n",
      "standard deviation :0.0\n",
      "UTO\n",
      "loading test dataset\n",
      "test MDD dataset size : 62\n",
      "concating train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataset\n",
      "train MDD dataset size : 193\n",
      "concating train dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehm4b7jq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47943<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210501_005346-ehm4b7jq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210501_005346-ehm4b7jq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>SITE</td><td>COI</td></tr><tr><td>epoch_cv</td><td>0</td></tr><tr><td>train_Loss_cv</td><td>0.00551</td></tr><tr><td>train_acc_cv</td><td>96.61017</td></tr><tr><td>_runtime</td><td>983</td></tr><tr><td>_timestamp</td><td>1619824210</td></tr><tr><td>_step</td><td>20</td></tr><tr><td>validation_acc_cv</td><td>100.0</td></tr><tr><td>validation_Loss_cv</td><td>0.0</td></tr><tr><td>learning rate</td><td>0.001</td></tr><tr><td>test_Acc_CV</td><td>100.0</td></tr><tr><td>test_Loss_CV</td><td>0.0</td></tr><tr><td>test_balanced_Acc_CV</td><td>1.0</td></tr><tr><td>test_F1_score_CV</td><td>1.0</td></tr><tr><td>test_balanced_Acc_Average</td><td>100.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch_cv</td><td>▁▁▁▁▁</td></tr><tr><td>train_Loss_cv</td><td>▃▁▁█▁</td></tr><tr><td>train_acc_cv</td><td>█▁▁▄▁</td></tr><tr><td>_runtime</td><td>▁▁▂▂▄▄▅▅▇▇▇▇▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▄▄▅▅▇▇▇▇▇▇▇▇█████</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>validation_acc_cv</td><td>▁▁▁▁▁</td></tr><tr><td>validation_Loss_cv</td><td>▆▁▁█▁</td></tr><tr><td>learning rate</td><td>▁▁▁▁▁</td></tr><tr><td>test_Acc_CV</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_Loss_CV</td><td>██▁▁▁▁▇▇▁▁</td></tr><tr><td>test_balanced_Acc_CV</td><td>▁▁▁▁▁</td></tr><tr><td>test_F1_score_CV</td><td>▁▁▁▁▁▁</td></tr><tr><td>test_balanced_Acc_Average</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">light-flower-5</strong>: <a href=\"https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/ehm4b7jq\" target=\"_blank\">https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/ehm4b7jq</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:ehm4b7jq). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">whole-sun-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho\" target=\"_blank\">https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/3v27se0v\" target=\"_blank\">https://wandb.ai/zahramhn/Leave-one-site-out-japanese-reho/runs/3v27se0v</a><br/>\n",
       "                Run data is saved locally in <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210501_011020-3v27se0v</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.005357299153040962\n",
      "train Acc= 96.75324675324676\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 3.2347506466703426e-06\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 0: 100 %\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n",
      "train loss= 0.004386184571425297\n",
      "train Acc= 100.0\n",
      "Training process has finished.\n",
      "Starting testing\n",
      "valid_acc :100.0\n",
      "valid_loss: 1.9539277590568118e-07\n",
      "Saving best valid -trained model.\n",
      "validation process has finished\n",
      "Accuracy for fold 1: 100 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "*********Starting epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5001a111d4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# Does the update , gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for site_num, site in enumerate(MDD_sites):\n",
    "    site_name= site['site'][0]\n",
    "    \n",
    "    print(site_name)\n",
    "    ##################### load test ######################\n",
    "    print('loading test dataset')\n",
    "    \n",
    "    ID_test_MDD=[]\n",
    "    for i in range(len(site['participants_id'])):\n",
    "        ID_test_MDD.append(site['participants_id'][i])\n",
    "    print(f'test MDD dataset size : {len(ID_test_MDD)}')\n",
    "    \n",
    "    Control_test= Control_sites[site_num] \n",
    "    ID_test_Control=[]\n",
    "    for j in range(len(Control_test['participants_id'])):\n",
    "        ID_test_Control.append(Control_test['participants_id'][j])\n",
    "    print(f'test Control dataset size : {len(ID_test_Control)}')\n",
    "    \n",
    "    test_site_mdd_file_path = [mdd_base_path + f'ReHo_z_{test_ids_MDD}.nii' for test_ids_MDD in ID_test_MDD]\n",
    "    #test_site_control_file_path = [control_base_path + f'ReHo_z_{test_ids_Control}.nii' for test_ids_Control in ID_test_Control]\n",
    "\n",
    "    mdd_test_site_reho_load = [[nib.load(m).get_fdata(),1] for m in test_site_mdd_file_path]\n",
    "    #control_test_site_reho_load = [[nib.load(c).get_fdata(),0] for c in test_site_control_file_path]\n",
    "    \n",
    "    # prepare test data for feeding to network\n",
    "    print('concating train dataset')\n",
    "    test_dataset_cv =  preparaing_summary_measure(mdd_test_site_reho_load, mdd_test_site_reho_load) \n",
    "\n",
    "    ##################### load train ######################\n",
    "    print('loading train dataset')\n",
    "    \n",
    "    ID_train_MDD=[]\n",
    "    for ids in MDD_sites_concat['participants_id']:\n",
    "        if ids not in ID_test_MDD:\n",
    "            ID_train_MDD.append(ids)\n",
    "    print(f'train MDD dataset size : {len(ID_train_MDD)}')\n",
    "    \n",
    "    ID_train_Control=[]\n",
    "    for ids in Control_sites_concat['participants_id']:\n",
    "        if ids not in ID_test_Control:\n",
    "            ID_train_Control.append(ids)\n",
    "    print(f'train Control dataset size : {len(ID_train_Control)}')\n",
    "            \n",
    "    \n",
    "    train_site_mdd_file_path = [mdd_base_path + f'ReHo_z_{train_ids_MDD}.nii' for train_ids_MDD in ID_train_MDD]\n",
    "    #train_site_control_file_path = [control_base_path + f'ReHo_z_{train_ids_Control}.nii' for train_ids_Control in ID_train_Control]\n",
    "    \n",
    "    mdd_train_site_reho_load = [[nib.load(m).get_fdata(),1] for m in train_site_mdd_file_path]\n",
    "    #control_train_site_reho_load = [[nib.load(c).get_fdata(),0] for c in train_site_control_file_path]\n",
    "    \n",
    "    # prepare train data for feeding to network\n",
    "    print('concating train dataset')\n",
    "    train_dataset_cv =  preparaing_summary_measure(mdd_train_site_reho_load,mdd_train_site_reho_load)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    \n",
    "    wandb.init(project='Leave-one-site-out-japanese-reho')\n",
    "\n",
    "\n",
    "    ############################### train and validation loop CV  ########################################\n",
    "    k_folds = 5\n",
    "    kfold_results = {}\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    #torch.manual_seed(42)\n",
    "    num_epochs = 1\n",
    "    batch_size = 10\n",
    "    learning_rate= 0.001\n",
    "\n",
    "\n",
    "    #Define a Loss function \n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_dataset_cv)):\n",
    "        best_loss_cv= None\n",
    "\n",
    "        print(f\"FOLD {fold}\\n--------------------------------\")\n",
    "\n",
    "        # Sample elements randomly from a given list of ids,\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size, sampler=train_subsampler)\n",
    "        valid_loader = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size, sampler=valid_subsampler)\n",
    "\n",
    "        #define network\n",
    "        network = ConvNet()\n",
    "        network = network.double()\n",
    "\n",
    "        # create our optimizer\n",
    "        optimizer = optim.SGD(network.parameters(), momentum=0.9, lr = learning_rate, weight_decay=1e-3)\n",
    "\n",
    "        # in the training loop:\n",
    "\n",
    "        network.train() # prepare model for training\n",
    "\n",
    "        for epoch in range(0, num_epochs):\n",
    "            print(f'*********Starting epoch {epoch+1}')\n",
    "\n",
    "            train_loss_cv = 0\n",
    "            total =0\n",
    "            correct=0\n",
    "\n",
    "            # train model/network \n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                #print(f'train {i}')\n",
    "\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "\n",
    "                # zero the gradient buffers\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                # print(outputs.size)\n",
    "                outputss=outputs.squeeze(1) #### [10,1] ---> [10]\n",
    "\n",
    "                # prediction \n",
    "                predicted = outputss.data > 0.0\n",
    "\n",
    "                labels=labels.double()\n",
    "\n",
    "                #calcuate loss/error\n",
    "                loss = loss_function(outputss, labels)\n",
    "\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Does the update , gradient descent\n",
    "                optimizer.step() \n",
    "\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                train_loss_cv += loss.item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            wandb.log({\"epoch_cv\": epoch , \"train_Loss_cv\": train_loss_cv/total, \"train_acc_cv\": 100 * correct / total })\n",
    "\n",
    "            print(f'train loss= {train_loss_cv/total}')\n",
    "            print(f'train Acc= {100 * correct / total}')\n",
    "\n",
    "\n",
    "            print('Training process has finished.')\n",
    "            print('Starting testing')\n",
    "\n",
    "            # validate the network using the validation data, for this fold\n",
    "            correct= 0\n",
    "            total = 0\n",
    "            valid_loss_cv=0\n",
    "\n",
    "            network.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(valid_loader, 0):\n",
    "                    #print(f'test {i}')\n",
    "\n",
    "                    inputs, lables = data\n",
    "\n",
    "                    outputs = network(inputs)\n",
    "\n",
    "                    outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "                    lables=lables.double()\n",
    "\n",
    "                    # prediction \n",
    "                    predicted = outputss.data > 0.0\n",
    "\n",
    "                    loss = loss_function(outputss, lables)\n",
    "\n",
    "                    valid_loss_cv += loss.item()\n",
    "                    total += lables.size(0)\n",
    "                    correct += (predicted == lables).sum().item()\n",
    "\n",
    "                wandb.log({ \"validation_acc_cv\": 100 * correct /total, \"validation_Loss_cv\": valid_loss_cv/total })\n",
    "\n",
    "                current_valid_loss_cv = valid_loss_cv/total\n",
    "\n",
    "                print(f'valid_acc :{100 * correct /total}')\n",
    "                print(f'valid_loss: {valid_loss_cv/total}')\n",
    "\n",
    "                if not best_loss_cv or best_loss_cv > current_valid_loss_cv:\n",
    "                    best_loss_cv = current_valid_loss_cv\n",
    "\n",
    "                    print('Saving best valid -trained model.')\n",
    "                    path_best_loss_reho = f'/data/zmohaghegh/TempStats_3D-CNN/leave_one_site_out_best_model_reho/model-japanese_best-reho-fold-{fold}.pth'\n",
    "                    torch.save(network.state_dict(), path_best_loss_reho )\n",
    "\n",
    "                print('validation process has finished')\n",
    "\n",
    "        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "        print('--------------------------------')\n",
    "\n",
    "        kfold_results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "\n",
    "\n",
    "    ##############################Result of Validation for each fold ##################################\n",
    "\n",
    "    print(f\"K-FOLD CROSS VALIDATION RESULTS FOR japanese reho {k_folds} FOLDS\\n--------------------------------\")\n",
    "    _sum = 0.0\n",
    "\n",
    "    for key, value in kfold_results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        _sum += value\n",
    "\n",
    "    print(f'Average: {_sum/len(kfold_results.items())} %')\n",
    "\n",
    "    ############################ ### Test loop for Cross validation  #############################################\n",
    "\n",
    "    #wandb.init(project='Leave-one-site-out-japanese-reho')\n",
    "    \n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset_cv , batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    bal_acc_fold=[]\n",
    "    F1_score_fold=[]\n",
    "\n",
    "    for k in np.arange(5): \n",
    "\n",
    "        print(f'Start TEST FOR FOLD {k}')\n",
    "\n",
    "        path_fold = f'/data/zmohaghegh/TempStats_3D-CNN/leave_one_site_out_best_model_reho/model-japanese_best-reho-fold-{k}.pth'\n",
    "\n",
    "        network.load_state_dict(torch.load(path_fold))\n",
    "\n",
    "        test_loss_cv=0\n",
    "        total = 0\n",
    "        correct=0\n",
    "\n",
    "        F1_labels=[]\n",
    "        F1_pred=[]\n",
    "\n",
    "        network.eval() # preoare model for test and evaluation\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #print('Start testing CV...')\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                #print(f'test {i}')\n",
    "\n",
    "                inputs, lables = data\n",
    "\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                lables=lables.double()\n",
    "                outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "\n",
    "                #do the prediction \n",
    "                predicted = outputss.data > 0.0\n",
    "\n",
    "                #calculate loss\n",
    "                loss = loss_function(outputss, lables)\n",
    "\n",
    "                test_loss_cv += loss.item()\n",
    "                correct += (predicted == lables).sum().item()\n",
    "                total += lables.size(0)\n",
    "\n",
    "                if i==0:\n",
    "                    F1_labels=lables.int().numpy()\n",
    "                    F1_pred=predicted.int().numpy()\n",
    "                else:\n",
    "                    F1_labels= np.concatenate((F1_labels, lables.int().numpy()))\n",
    "                    F1_pred = np.concatenate((F1_pred, predicted.int().numpy()))\n",
    "\n",
    "            wandb.log({ \"test_Acc_CV\": 100 * correct /total  , \"test_Loss_CV\": test_loss_cv/total })\n",
    "\n",
    "            acc = accuracy_score(F1_labels, F1_pred)\n",
    "            bal_acc= balanced_accuracy_score(F1_labels, F1_pred)\n",
    "\n",
    "            F1_Score = f1_score(F1_labels, F1_pred, average='weighted')\n",
    "            #tn, fp, fn, tp = confusion_matrix(F1_labels, F1_pred).ravel()\n",
    "\n",
    "            F1_score_fold.append(F1_Score)\n",
    "            bal_acc_fold.append(bal_acc)\n",
    "\n",
    "            #PLotting Confusion matrix and ROC curve\n",
    "\n",
    "            #conf_matrix = confusion_matrix(F1_labels, F1_pred)\n",
    "            #conf_matrix_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "\n",
    "            #fp_rate, tp_rate, threshold = roc_curve(F1_labels, F1_pred)\n",
    "            #ROC_display = RocCurveDisplay(fpr=fp_rate, tpr=tp_rate).plot()\n",
    "\n",
    "\n",
    "            wandb.log({ \"test_balanced_Acc_CV\": bal_acc , \"test_Acc_CV\": 100 * correct /total, \"test_F1_score_CV\": F1_Score , \"test_Loss_CV\": test_loss_cv/total })\n",
    "\n",
    "            print(f'test_Acc_CV\": {100 * correct /total}')\n",
    "            print(f'F1_score CV ReHo :{F1_Score}')\n",
    "            print(f'Balanced ACC CV ReHo :{bal_acc}')\n",
    "            print(f'Loss CV ReHo : {test_loss_cv/total}')\n",
    "\n",
    "\n",
    "\n",
    "    ############################ ### Result of test loop average  #############################################       \n",
    "    F1_score_avg= sum(F1_score_fold)/len(F1_score_fold)\n",
    "    F1_score_std= statistics.pstdev(F1_score_fold)\n",
    "    bal_acc_avg = 100 * (sum(bal_acc_fold)/len(bal_acc_fold))\n",
    "    bal_acc_std = 100* statistics.pstdev(bal_acc_fold)\n",
    "\n",
    "    print(f' Site name : {site_name}')\n",
    "    print(f' @@@@@@@@@@ Average Balance ACC japan ReHo = {bal_acc_avg}')\n",
    "    print(f'standard deviation :{bal_acc_std}')\n",
    "    print(f' @@@@@@@@@@@ Average F1_score japan ReHo = { F1_score_avg}')\n",
    "    print(f'standard deviation :{F1_score_std}')\n",
    "    \n",
    "    wandb.log({ \"SITE\": site_name, \"test_balanced_Acc_Average\": bal_acc_avg, \"test_F1_score_CV\":F1_score_avg})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese subset_ main thesis project -ReHo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start of classsification using Reho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 255\n"
     ]
    }
   ],
   "source": [
    "control_base_path = '/dbstore/zmohaghegh/Japanese_subset/summary_measures/Control_reho/ReHo_Normalised_z/'\n",
    "mdd_base_path = '/dbstore/zmohaghegh/Japanese_subset/summary_measures/MDD_reho/ReHo_Normalised_z/'\n",
    "\n",
    "#define data path for control and MDD seprately\n",
    "\n",
    "control_files_path = [control_base_path + c for c in os.listdir(control_base_path) if c.startswith('R')]\n",
    "mdd_files_path = [mdd_base_path + m for m in os.listdir(mdd_base_path) if m.startswith('R')]\n",
    "print(len(control_files_path), len(mdd_files_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data with nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data seprately and set a label column for Control=0 and MDD=1 nested list \n",
    "\n",
    "control_dataset_reho_zero = [[nib.load(c).get_fdata(),0] for c in control_files_path]\n",
    "mdd_dataset_reho_zero = [[nib.load(m).get_fdata(),1] for m in mdd_files_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define numpy array for doing the zero nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "0\n",
      "(91, 109, 91)\n",
      "251\n",
      "(91, 109, 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n",
      "/data/zmohaghegh/venv/lib64/python3.6/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_reho_zero_nan2= control_dataset_reho_zero\n",
    "mdd_reho_zero_nan2 = mdd_dataset_reho_zero\n",
    "\n",
    "print(len(mdd_reho_zero_nan2))\n",
    "print(control_reho_zero_nan2[0][1]) ### subject label \n",
    "print(mdd_reho_zero_nan2[0][0].shape) ### subject nifti -summary measure\n",
    "\n",
    "\n",
    "mdd_reho_zero_nan3 = np.array(mdd_reho_zero_nan2)\n",
    "control_reho_zero_nan3 =np.array(control_reho_zero_nan2)\n",
    "\n",
    "\n",
    "#####control\n",
    "print(len(control_reho_zero_nan2))\n",
    "print(control_reho_zero_nan2[0][0].shape)\n",
    "\n",
    "control_reho_zero_nan3[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "(2,)\n",
      "2\n",
      "(2,)\n",
      "(91, 109, 91)\n"
     ]
    }
   ],
   "source": [
    "print(control_reho_zero_nan3.shape[0])\n",
    "print(control_reho_zero_nan3[0].shape)\n",
    "print(control_reho_zero_nan3.shape[1])\n",
    "print(control_reho_zero_nan3[1].shape)\n",
    "print(control_reho_zero_nan3[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdd_dataset_reho_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(control_dataset_reho_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdd_dataset_reho_zero[0])\n",
    "len(control_dataset_reho_zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdd_dataset_reho_zero[0][0].shape\n",
    "control_dataset_reho_zero[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([            nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "       -5.60656250e-01, -2.18471512e-01,  2.65964437e-02,  1.17912032e-01,\n",
       "       -5.86777329e-01, -1.27932799e+00, -5.91229200e-01,  3.55972379e-01,\n",
       "        4.87391263e-01,  1.03013563e+00,  1.23969424e+00,  1.46534503e+00,\n",
       "        1.19105673e+00,  9.16325331e-01,  9.76231158e-01,  9.03224111e-01,\n",
       "        5.72811186e-01, -9.82609694e-04, -6.33052528e-01, -8.04333329e-01,\n",
       "       -8.95923316e-01, -9.37934101e-01, -7.56355286e-01, -2.29388937e-01,\n",
       "        5.67567110e-01,  8.79382014e-01,  4.35421288e-01, -2.55543739e-01,\n",
       "       -2.71465331e-01,  1.12094735e-04,  8.33752751e-01,  7.89981782e-01,\n",
       "        1.64858356e-01, -1.56577587e-01, -1.83763757e-01, -3.08324248e-01,\n",
       "       -1.22617386e-01,  5.84013537e-02, -4.67320681e-01, -1.08100688e+00,\n",
       "       -7.44391143e-01, -1.69124708e-01,  2.59951442e-01,  4.71335471e-01,\n",
       "        5.45791864e-01,  5.67753077e-01,  6.99446380e-01,  8.36949110e-01,\n",
       "        7.54920900e-01,  5.83105147e-01,  1.34545162e-01,  2.50113338e-01,\n",
       "        4.10586119e-01,  9.99001741e-01,  7.97814190e-01,  5.79920769e-01,\n",
       "        2.86084861e-01,  6.50586545e-01,  1.19449568e+00,  1.23251760e+00,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_dataset_reho_zero[190][0][:,34,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([            nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "        1.45227027e+00,  1.58096850e+00,  1.55474269e+00,  1.78304231e+00,\n",
       "        1.76300514e+00,  1.83052921e+00,  1.73060858e+00,  1.49151731e+00,\n",
       "        8.38405073e-01,  1.51621774e-01,  5.58712631e-02,  7.96048989e-05,\n",
       "       -4.80724126e-01, -2.73976892e-01, -4.16711837e-01, -7.36057162e-01,\n",
       "       -9.18137878e-02,  1.02721065e-01, -2.07152441e-02, -1.40896872e-01,\n",
       "       -1.06029892e+00, -1.56802857e+00, -1.51698637e+00,  3.41506273e-01,\n",
       "        6.67224765e-01,  3.86154920e-01,  7.52673447e-02,  5.10709226e-01,\n",
       "        1.27161467e+00,  7.03470409e-01,  8.21025670e-01,  6.33770823e-01,\n",
       "       -2.22559988e-01, -1.43178129e+00, -1.59917831e+00, -1.46645176e+00,\n",
       "       -1.42161059e+00, -1.59846115e+00, -1.43462229e+00, -1.10574567e+00,\n",
       "       -4.14674819e-01, -8.34896713e-02,  1.02813733e+00,  8.25832844e-01,\n",
       "        3.50431830e-01,  6.54603958e-01,  7.78769374e-01,  1.68393207e+00,\n",
       "        1.70185030e+00,  1.32389045e+00,  1.35253930e+00,  1.62199521e+00,\n",
       "        1.45914495e+00,  1.38224781e+00,  1.37869036e+00,  1.41899765e+00,\n",
       "        1.36579216e+00,  1.06594896e+00,  2.45684996e-01,  2.56595492e-01,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdd_dataset_reho_zero[100][0][:,34,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.97561115, -0.41167933,  0.45215327,  1.35208154,\n",
       "        1.81005454,  1.64219189,  0.75563222, -0.85597694, -1.74802983,\n",
       "       -1.14755046, -0.46927831,  0.1376407 ,  0.55535668, -0.61553514,\n",
       "       -1.1796025 , -0.36322761, -0.56221068, -0.04582196,  1.29621768,\n",
       "       -1.07614529, -1.15474641, -1.40290833, -0.10318688,  1.10218644,\n",
       "       -0.21096501, -0.70668888, -0.83129627,  0.01479594,  0.39268684,\n",
       "        0.06674624, -0.14645448, -0.20309636,  0.82269102, -1.23148084,\n",
       "       -1.41650319, -1.12033212, -0.8095482 , -0.38718724, -0.07395672,\n",
       "       -0.5001781 , -0.74937856, -0.21215707,  0.18034168,  0.32574278,\n",
       "       -0.03737804,  0.16908635,  0.25720319,  0.50484079,  1.33211768,\n",
       "        1.79723573,  1.65976012,  1.12066436,  0.90531272,  0.17280258,\n",
       "       -0.15098582, -0.49954042, -0.6192193 , -0.94158643, -1.13055968,\n",
       "       -1.03592718,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example control\n",
    "zero_nan_control_800= np.nan_to_num(control_dataset_reho_zero[100][0],copy=True)\n",
    "zero_nan_control_800[:,34,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.29152313,  0.44130495,  0.96773052,  0.73992622,\n",
       "        0.2600117 , -0.74334651, -0.90461767, -0.20272742,  0.47446784,\n",
       "        0.54080647,  0.2975871 ,  0.25276366, -0.06263041,  0.01617018,\n",
       "        0.26543203,  0.18063113,  0.06255207,  0.17151089, -0.14120804,\n",
       "       -0.56722862, -0.31063247, -0.25900012, -0.12108749, -0.64518219,\n",
       "       -1.4728694 , -1.89632857, -1.14513648, -0.17089775,  0.44840688,\n",
       "       -0.37902629, -0.96976787, -1.1759299 , -0.70549959, -0.72158879,\n",
       "       -0.49296591,  0.06177398,  0.81728059,  0.86775333,  1.03077555,\n",
       "        1.16670609,  1.47919154,  1.60115576,  1.42342389,  0.93247628,\n",
       "        0.48033294,  0.09809522, -0.33412564,  0.20377555,  0.15091807,\n",
       "       -0.15625139, -0.15019156, -0.35716426,  0.00880462,  0.26110956,\n",
       "        0.32117105,  1.26215851,  1.72534251,  1.21366763,  0.81217551,\n",
       "        1.55748332,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example mdd\n",
    "zero_nan_mdd_700= np.nan_to_num(mdd_dataset_reho_zero[200][0],copy=True)\n",
    "zero_nan_mdd_700[:,34,56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 2)\n",
      "(91, 109, 91)\n",
      "0\n",
      "0\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#zero_nan_control\n",
    "print(control_reho_zero_nan3.shape)\n",
    "print(control_reho_zero_nan3[0][0].shape)\n",
    "print(control_reho_zero_nan3[0][1])\n",
    "print(control_reho_zero_nan3[100][1])\n",
    "print(control_reho_zero_nan3[0].shape)\n",
    "print(type(control_reho_zero_nan3))\n",
    "\n",
    "\n",
    "for i in range(len(control_dataset_reho_zero)):    \n",
    "    control_reho_zero_nan3[i][0] =np.nan_to_num(control_dataset_reho_zero[i][0],copy=True)\n",
    "    control_reho_zero_nan3[i][1] =np.nan_to_num(control_dataset_reho_zero[i][1],copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 2)\n",
      "(91, 109, 91)\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.40634489,  1.16506803,  1.24199009,  1.5856874 ,\n",
       "        1.90874386,  2.1039741 ,  1.97037923,  1.63948965,  0.0582762 ,\n",
       "       -0.76295084,  0.14982255,  0.87426162,  0.01851538, -0.89479417,\n",
       "       -1.10983622, -0.31956229,  0.27067229, -1.15117979, -1.14975691,\n",
       "       -1.04158092, -1.3259232 , -1.15197194, -0.96472198, -0.29523233,\n",
       "        0.71875554,  1.56608725,  1.57721782,  0.9202832 ,  0.77033854,\n",
       "        0.80137569,  1.56249702,  1.45230246,  1.14340329,  0.7310912 ,\n",
       "        0.4318586 , -0.21386313, -0.24157619, -0.61160183, -0.85414231,\n",
       "       -0.78928238, -0.11784432, -0.61842167, -0.80912924, -0.50917006,\n",
       "       -0.21775049, -0.60851109, -0.98969173, -0.21536504,  0.30669811,\n",
       "        0.5547269 , -0.16132496, -0.08607935,  0.20520933,  0.86126506,\n",
       "        1.51655018,  1.65980136,  1.81314564,  1.95989585,  2.04652119,\n",
       "        2.08455515,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(control_reho_zero_nan3.shape)\n",
    "print(control_reho_zero_nan3[0][0].shape)\n",
    "print(control_reho_zero_nan3[0][1]) ## mdd label for control=0\n",
    "print(control_reho_zero_nan3[110][1])\n",
    "control_reho_zero_nan3[110][0][:,34,56]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 2)\n",
      "(91, 109, 91)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(255, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zero_nan_mdd\n",
    "\n",
    "print(mdd_reho_zero_nan3.shape)\n",
    "print(mdd_reho_zero_nan3[0][0].shape)\n",
    "print(type(mdd_reho_zero_nan3))\n",
    "\n",
    "for i in range(len(mdd_dataset_reho_zero)):    \n",
    "    mdd_reho_zero_nan3[i][0] =np.nan_to_num(mdd_dataset_reho_zero[i][0],copy=True)\n",
    "    mdd_reho_zero_nan3[i][1] =np.nan_to_num(mdd_dataset_reho_zero[i][1],copy=True)\n",
    "\n",
    "mdd_reho_zero_nan3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 2)\n",
      "(91, 109, 91)\n",
      "(2,)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.15539241,  1.09352338,  1.0503757 ,  1.43768322,\n",
       "        1.51124728,  1.43333316,  0.91985607,  0.08519863,  0.00556017,\n",
       "       -0.52437329, -0.77414989, -1.12355745, -1.14360178, -1.20118988,\n",
       "       -0.74124074, -0.30203831, -0.13550924, -0.58327138, -0.92846113,\n",
       "       -0.50423551, -0.61148393, -0.59212548, -0.61149406, -0.64308512,\n",
       "       -0.73825824, -0.75959575, -0.96736431, -0.5596289 , -0.66086632,\n",
       "       -0.96182823, -1.21448779, -1.07356155, -0.8627215 , -0.84807622,\n",
       "       -1.20948648, -1.15942013, -0.94385028, -0.81308424, -1.16578507,\n",
       "       -1.2775619 , -1.09162247, -1.41870797, -0.78159875,  0.2320734 ,\n",
       "        0.72339165,  0.75407511,  0.96091902,  0.93616128,  1.08235204,\n",
       "        1.20815241,  1.57756925,  1.41936743,  1.42582369,  1.35411692,\n",
       "        1.0311377 ,  1.0431962 ,  0.80032986,  0.46558991, -0.09556559,\n",
       "        0.30094936,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mdd_reho_zero_nan3.shape)\n",
    "\n",
    "mdd_reho_zero_nan3.shape\n",
    "print(mdd_reho_zero_nan3[0][0].shape)\n",
    "print(mdd_reho_zero_nan3[0].shape)\n",
    "print(mdd_reho_zero_nan3[0][1])\n",
    "mdd_reho_zero_nan3[210][0][:,34,56]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and add 1 channel to dimension of data 'without nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.reshape(control_dataset[0][0], (1, 91, 109, 91))\n",
    "\n",
    "control_reho_4d_zero_nan = [[np.reshape(c[0], (1, 91, 109, 91)), c[1]] for c in control_reho_zero_nan3]\n",
    "mdd_reho_4d_zero_nan = [[np.reshape(m[0], (1, 91, 109, 91)), m[1]] for m in mdd_reho_zero_nan3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 91, 109, 91)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mdd_reho_4d_zero_nan[200][0].shape)\n",
    "mdd_reho_4d_zero_nan[200][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 91, 109, 91)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(control_reho_4d_zero_nan[100][0].shape)\n",
    "control_reho_4d_zero_nan[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdd_reho_4d_zero_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdd_reho_4d_zero_nan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat data for control and MDD without nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 91, 109, 91)\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.30531952,  0.3058778 ,  0.32962349,  0.4114182 ,\n",
       "        0.24398486, -0.0345251 , -0.01915896, -0.16323367, -0.10207997,\n",
       "       -0.2218703 , -0.73692161, -0.77802765, -0.31200635,  0.11806874,\n",
       "        0.30682969,  0.3686862 ,  0.40877363,  0.27833337,  0.36288431,\n",
       "        0.08952609,  0.5410139 , -0.1906241 , -0.4622815 , -0.12508819,\n",
       "       -0.00884558,  0.09208355, -0.69580549, -1.54331946, -1.02894855,\n",
       "       -0.24026568, -0.95924526, -2.21315384, -2.32113814, -2.36360312,\n",
       "       -2.51081634, -1.85549366, -2.15758109, -2.67912054, -1.75750256,\n",
       "       -0.96856028, -1.57662106, -1.86215401, -1.16792428, -0.79216236,\n",
       "       -1.09158075, -0.95693052, -1.01139581, -0.64076531, -0.27808905,\n",
       "        0.27005121,  1.03516877,  0.41657588, -0.41263399, -0.51738447,\n",
       "       -0.52412993, -0.66663504, -0.68396509, -0.06062748,  0.15602991,\n",
       "       -0.19889852, -0.52108008, -0.81781334,  0.02819817,  0.96908629,\n",
       "        1.85588574,  2.31624317,  2.2362082 ,  2.14120173,  2.12160683,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_japan_reho_zero_nan= ConcatDataset([control_reho_4d_zero_nan, mdd_reho_4d_zero_nan])\n",
    "\n",
    "print(dataset_japan_reho_zero_nan[0][0].shape)\n",
    "print(dataset_japan_reho_zero_nan[400][1])\n",
    "print(dataset_japan_reho_zero_nan[210][1])\n",
    "\n",
    "\n",
    "dataset_japan_reho_zero_nan[400][0][0,:,54,45]\n",
    "\n",
    "#subject number=1500, mdd label,reho value for voxels along axial with y ,z dimention = 54,45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_japan_reho_zero_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_japan_reho_zero_nan[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 91, 109, 91)\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 91, 109, 91)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_japan_reho_zero_nan[200][0].shape) # subject 800 label MDD =0\n",
    "print(dataset_japan_reho_zero_nan[400][1]) # subject 1500 label MDD =1 \n",
    "\n",
    "#problem solved : mdd_zero_nan label have problem : instead of 1 integer we have an array (91,109,91)\n",
    "\n",
    "print(dataset_japan_reho_zero_nan[0][1])\n",
    "print(dataset_japan_reho_zero_nan[400][1])\n",
    "dataset_japan_reho_zero_nan[400][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Convolutional Neural Network : BASED ON https://www.biorxiv.org/content/10.1101/2019.12.17.879346v1\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.downsample = nn.AvgPool3d(2, stride=2, padding=0)\n",
    "        \n",
    "        self.CNNlayer = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv3d(64, 16, kernel_size=3, stride=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "        \n",
    "        self.flat1 = nn.Linear(160000, 16)   \n",
    "        self.flat2 = nn.Linear(16, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x=self.downsample(x)\n",
    "        #print(f'avg-pool: {x.size()}\\n----------')\n",
    "        #print(f'number of nan in this layer = {torch.isnan(x).sum()}')\n",
    "        \n",
    "        x=self.CNNlayer(x)\n",
    "        #print(f'convolution1+2+maxpool: {x.size()} \\n----------')\n",
    "        \n",
    "        x=x.reshape(x.size(0), -1)\n",
    "        #print(f'reshape after cnn: {x.size()}\\n----------')\n",
    "        \n",
    "        x=F.elu(self.flat1(x))\n",
    "        #print(f'fully-connected1: {x.size()}\\n----------')\n",
    "                    \n",
    "        x=self.flat2(x)\n",
    "        #print(f'fully-connected2: {x.size()}\\n----------')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lunch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/zmohaghegh/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 390734ff44d817dbba59927d4eb542e564627b3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test (without CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spilit data to train-test and validation\n",
    "\n",
    "dataset_size= len(dataset_ukbb_reho_zero_nan)\n",
    "train_count = int(0.7 * dataset_size) \n",
    "valid_count= int(0.2 * dataset_size)\n",
    "test_count = dataset_size - train_count -valid_count\n",
    "\n",
    "print(dataset_size)\n",
    "print(train_count)\n",
    "print(valid_count)\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project='mdd-japanese-reho')  \n",
    "\n",
    "#define the newtwork \n",
    "network = ConvNet()\n",
    "network = network.double()\n",
    "\n",
    "#set hyper parameter\n",
    "learning_rate= 0.001\n",
    "num_epochs = 10\n",
    "BATCH_SIZE = 10\n",
    "best_acc= None\n",
    "\n",
    "# Define a Loss function \n",
    "loss_function = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.SGD(network.parameters(), momentum=0.9, lr = learning_rate, weight_decay=1e-3)\n",
    "\n",
    "# load data splits\n",
    "train_dataset, test_dataset ,valid_dataset = torch.utils.data.random_split(dataset_japan_reho_zero_nan, (train_count, test_count,valid_count))\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  \n",
    "test_dataset_loader  = torch.utils.data.DataLoader(test_dataset , batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_dataset_loader  = torch.utils.data.DataLoader(valid_dataset , batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "#dataloaders = {'train': train_dataset_loader, 'test': test_dataset_loader, 'valid': valid_dataset_loader}\n",
    "\n",
    "network.train()\n",
    "# start training loop:\n",
    "for epoch in range(0, num_epochs):\n",
    "    print(f'*********Starting epoch {epoch+1}')\n",
    "    \n",
    "    train_loss = 0\n",
    "    total =0 \n",
    "    correct = 0\n",
    "    \n",
    "    # train model/network \n",
    "    for i, data in enumerate(train_dataset_loader, 0):\n",
    "        #print(f'train {i}')\n",
    "            \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "            \n",
    "        # zero the gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # forward pass\n",
    "        outputs = network(inputs)\n",
    "    \n",
    "        outputss = outputs.squeeze(1) #### [10,1] ---> [10]\n",
    "        \n",
    "        #nloss requires a 1-D tensor of class indices of the target (outputs in your case) but not one-hot vectors of the target.\n",
    "        #The shape of the target should be [mini_batch_size], \n",
    "        #but in your case it happens to be [mini_batch_size,num_classes].\n",
    "        \n",
    "        predicted = outputss.data > 0.0 # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "        \n",
    "        labels = labels.double()\n",
    "        \n",
    "        #calcuate loss/error\n",
    "        loss = loss_function(outputss, labels)\n",
    "              \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "            \n",
    "        # Does the update , gradient descent\n",
    "        optimizer.step() \n",
    "            \n",
    "        #loss_list.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'acc_train :{100 * correct/total}')\n",
    "    print (f'loss_train = {train_loss/total}')\n",
    "    \n",
    "    wandb.log({ \"epoch\": epoch ,\"train_Loss\": train_loss/total, \"trian_acc\" :100 * correct /total })\n",
    "\n",
    "    \n",
    "    print('Training process has finished') \n",
    "\n",
    "    print('starting validation :')\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "    valid_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_dataset_loader, 0):\n",
    "            #print(f'valid {i}')\n",
    "            inputs, lables = data\n",
    "\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "            lables=lables.double()\n",
    "\n",
    "            # prediction \n",
    "            predicted = outputss.data > 0.0 # the loss function contain also a sigmoid layer <0 = false >0 =tru\n",
    "\n",
    "            loss = loss_function(outputss, lables)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            total += lables.size(0)\n",
    "            correct += (predicted == lables).sum().item()\n",
    "\n",
    "        print(f'acc_valid :{100 * correct/total}')\n",
    "        print (f'loss_valid = {valid_loss/total}')\n",
    "\n",
    "        wandb.log({ \"validation_acc\" :100 * correct /total, \"validation_Loss\": valid_loss/total })\n",
    "\n",
    "        current_valid_acc =100 * correct/total\n",
    "        if not best_acc or best_acc < current_valid_acc:\n",
    "            best_acc = current_valid_acc\n",
    "            \n",
    "            print('Saving trained model.')\n",
    "            PATH_best = './model-japanese-best-acc-reho.pth'\n",
    "            torch.save(network.state_dict(), PATH_best)\n",
    "        print('validation process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network using the test data without CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load(PATH_best))\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total= 0\n",
    "\n",
    "print('Starting testing')\n",
    "\n",
    "network.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataset_loader, 0):\n",
    "        #print(f'test {i}')\n",
    "        \n",
    "        inputs, lables = data\n",
    "        lables = lables.double() # loss function get only double not Long\n",
    "        \n",
    "        outputs = network(inputs)\n",
    "        #print(outputs.size())\n",
    "        \n",
    "        outputss = outputs.squeeze(1) #[10,1] ---> [10]\n",
    "        #print(outputss.size())\n",
    "        #print(lables.size())\n",
    "    \n",
    "        predicted = outputss.data > 0.0 # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "        \n",
    "        loss = loss_function(outputss, lables)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        total += lables.size(0)\n",
    "        correct += (predicted == lables).sum().item()\n",
    "        \n",
    "    wandb.log({ \"test_Accuracy\": 100 * correct /total  , \"test_Loss\": test_loss/total })\n",
    "\n",
    "    print(f'test_loss= {test_loss/total}')\n",
    "    print(f'test_acc= {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "#train_dataset_cv, test_dataset_cv = train_test_split(dataset_ukbb_reho_zero_nan, test_size=0.20, train_size=0.8,shuffle=True)\n",
    "#print(len(train_dataset_cv))\n",
    "#print(len(test_dataset_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgage[0].shape\n",
    "#image.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_japan_reho_zero_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=2, random_state=None, shuffle=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   8,   9,  11,  12,  13,  15,\n",
       "        16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "        29,  30,  31,  32,  33,  34,  35,  37,  38,  39,  40,  41,  42,\n",
       "        43,  45,  46,  48,  49,  50,  52,  53,  54,  55,  56,  59,  60,\n",
       "        61,  63,  64,  65,  66,  68,  69,  70,  71,  73,  74,  75,  76,\n",
       "        78,  79,  80,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "        96,  97,  98,  99, 101, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "       112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "       140, 141, 142, 144, 145, 146, 147, 149, 150, 152, 153, 154, 155,\n",
       "       157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171,\n",
       "       172, 173, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186,\n",
       "       188, 189, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202,\n",
       "       203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 216, 217, 219,\n",
       "       222, 223, 226, 227, 229, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "       239, 241, 242, 243, 244, 246, 248, 249, 250, 251, 252, 254, 255,\n",
       "       257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270,\n",
       "       271, 272, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 294, 295, 296, 297, 299, 300, 301,\n",
       "       302, 303, 304, 305, 306, 307, 308, 309, 311, 313, 314, 315, 316,\n",
       "       317, 318, 319, 322, 323, 324, 327, 328, 329, 330, 332, 333, 334,\n",
       "       335, 336, 337, 338, 339, 341, 343, 344, 345, 346, 347, 348, 349,\n",
       "       351, 353, 355, 356, 357, 358, 359, 360, 362, 363, 365, 366, 367,\n",
       "       368, 369, 370, 372, 374, 375, 376, 377, 378, 379, 381, 382, 383,\n",
       "       385, 386, 387, 389, 391, 393, 396, 398, 399, 401, 403, 404, 405,\n",
       "       406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418,\n",
       "       419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "       434, 436, 438, 439, 440, 442, 443, 444, 445, 446, 450, 451, 453,\n",
       "       454, 455, 456, 457, 460, 461, 462, 463, 464, 466, 468, 469, 470,\n",
       "       471, 473, 474, 475, 477, 481, 482, 483, 484, 485, 486, 487, 488,\n",
       "       489, 490, 491, 492, 494, 495, 496, 497, 499, 501, 502, 503, 504,\n",
       "       505])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-141f529edbf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_valid_folds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_japan_reho_zero_nan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib64/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"absolute value of index should not exceed dataset length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "train_valid_folds.split(dataset_japan_reho_zero_nan[train_ids][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and validation and test loop CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ftm99wr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17909<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210525_163418-2ftm99wr/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210525_163418-2ftm99wr/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dry-silence-65</strong>: <a href=\"https://wandb.ai/zahramhn/mdd-japanese-reho/runs/2ftm99wr\" target=\"_blank\">https://wandb.ai/zahramhn/mdd-japanese-reho/runs/2ftm99wr</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2ftm99wr). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">major-voice-66</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zahramhn/mdd-japanese-reho\" target=\"_blank\">https://wandb.ai/zahramhn/mdd-japanese-reho</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zahramhn/mdd-japanese-reho/runs/1zxmp6yh\" target=\"_blank\">https://wandb.ai/zahramhn/mdd-japanese-reho/runs/1zxmp6yh</a><br/>\n",
       "                Run data is saved locally in <code>/data/zmohaghegh/TempStats_3D-CNN/wandb/run-20210525_163531-1zxmp6yh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5  17  20  22  33  40  48  50  55  57  62  65  69  75  79  84  85  88\n",
      "  93  96  99 101 119 128 129 142 148 152 155 156 164 167 172 177 180 186\n",
      " 191 194 197 198 204 214 219 221 222 223 236 237 240 247 248 256 267 269\n",
      " 270 271 273 274 276 279 280 289 304 314 316 331 336 340 341 351 356 357\n",
      " 358 360 362 367 381 383 384 390 392 400 408 413 414 416 418 429 432 435\n",
      " 444 451 464 466 477 482 488 493 496 501 502 504]\n",
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'KFold' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-0f3448067fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_valid_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_valid_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_ids_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_valid_folds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_japan_reho_zero_nan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'KFold' has no len()"
     ]
    }
   ],
   "source": [
    "wandb.init(project='mdd-japanese-reho')  \n",
    "\n",
    "k_folds = 5\n",
    "kfold_results = {}\n",
    "\n",
    "# StratifiedKFold instead of KFOLD for balanced number of class for each fold\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)  \n",
    "\n",
    "#Hyper parameter\n",
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "learning_rate= 0.00001\n",
    "\n",
    "\n",
    "#Define a Loss function \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset_japan_reho_zero_nan)):\n",
    "    best_loss_cv= None\n",
    "    print(test_ids)\n",
    "    \n",
    "    print(f\"FOLD {fold}\\n--------------------------------\")\n",
    "    \n",
    "    train_valid_folds = KFold(n_splits=2, shuffle=True)\n",
    "    len(train_valid_folds)\n",
    "    \n",
    "    train_ids_new, valid_ids = list(train_valid_folds.split(dataset_japan_reho_zero_nan[train_ids]))[0]\n",
    "    \n",
    "    # Create training,testing and validation datasets\n",
    "    train_data = dataset_ukbb_reho_zero_nan[train_ids_new]\n",
    "    valid_data = dataset_ukbb_reho_zero_nan[valid_ids]\n",
    "    test_data = dataset_ukbb_reho_zero_nan[test_ids]\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids,\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids_new)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, sampler=valid_subsampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_data , batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    #define network\n",
    "    network = ConvNet()\n",
    "    network = network.double()\n",
    "    \n",
    "    # create our optimizer\n",
    "    optimizer = optim.SGD(network.parameters(), momentum=0.9, lr = learning_rate, weight_decay=1e-3)\n",
    "    \n",
    "    # in the training loop:\n",
    "    \n",
    "    network.train() # prepare model for training\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        print(f'*********Starting epoch {epoch+1}')\n",
    "        \n",
    "        train_loss_cv = 0\n",
    "        total =0\n",
    "        correct=0\n",
    "\n",
    "        # train model/network \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            #print(f'train {i}')\n",
    "            \n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = network(inputs)\n",
    "            \n",
    "            # print(outputs.size)\n",
    "            outputss=outputs.squeeze(1) #### [10,1] ---> [10]\n",
    "            \n",
    "            # prediction \n",
    "            predicted = outputss.data > 0.0 # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "            \n",
    "            labels=labels.double()\n",
    "            \n",
    "            #calcuate loss/error\n",
    "            loss = loss_function(outputss, labels)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Does the update , gradient descent\n",
    "            optimizer.step() \n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            train_loss_cv += loss.item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "        #wandb.log({ \"epoch_cv\": epoch , \"train_Loss_cv\": train_loss_cv/total, \"train_acc_cv\": 100 * correct / total })\n",
    "        \n",
    "        print(f'train loss= {train_loss_cv/total}')\n",
    "        print(f'train Acc= {100 * correct / total}')\n",
    "        \n",
    "        \n",
    "        print('Training process has finished.')\n",
    "        print('Starting testing')\n",
    "\n",
    "        # validate the network using the validation data, for this epoch\n",
    "        correct= 0\n",
    "        total = 0\n",
    "        valid_loss_cv=0\n",
    "        \n",
    "        network.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valid_loader, 0):\n",
    "                #print(f'test {i}')\n",
    "\n",
    "                inputs, lables = data\n",
    "\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "                lables=lables.double()\n",
    "\n",
    "                # prediction \n",
    "                predicted = outputss.data > 0.0 # the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "\n",
    "                loss = loss_function(outputss, lables)\n",
    "\n",
    "                valid_loss_cv += loss.item()\n",
    "                total += lables.size(0)\n",
    "                correct += (predicted == lables).sum().item()\n",
    "\n",
    "            #wandb.log({ f'validation_acc_fold_{fold}': 100 * correct /total, f'validation_Loss_fold_{fold}': valid_loss_cv/total})\n",
    "            \n",
    "            current_valid_loss_cv = 100 * correct /total\n",
    "            \n",
    "            print(f'valid_acc :{100 * correct /total}')\n",
    "            print(f'valid_loss: {valid_loss_cv/total}')\n",
    "                  \n",
    "            if not best_loss_cv or best_loss_cv < current_valid_loss_cv:\n",
    "                best_loss_cv = current_valid_loss_cv\n",
    "                best_network=network\n",
    "                \n",
    "                print('Saving best valid -trained model.')\n",
    "                path_best_loss_reho = f'/data/zmohaghegh/TempStats_3D-CNN/best_model_reho_japanese/model-japanese_best-reho-fold-{fold}.pth'\n",
    "                torch.save(network.state_dict(), path_best_loss_reho )\n",
    "                \n",
    "            print('validation process has finished')\n",
    "        \n",
    "    print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "    print('--------------------------------')\n",
    "    \n",
    "        \n",
    "    kfold_results[fold] = 100.0 * (correct / total)\n",
    "    \n",
    "    print(f\"K-FOLD CROSS VALIDATION RESULTS FOR japanese reho {k_folds} FOLDS\\n--------------------------------\")\n",
    "    _sum = 0.0\n",
    "\n",
    "    for key, value in kfold_results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        _sum += value\n",
    "\n",
    "    print(f'Average: {_sum/len(kfold_results.items())} %')\n",
    "    \n",
    "    \n",
    "    #start testing \n",
    "    print('start testing...')\n",
    "    \n",
    "    #network.load_state_dict(torch.load(path_fold))\n",
    "\n",
    "    test_loss_cv=0\n",
    "    total = 0\n",
    "    correct=0\n",
    "\n",
    "    F1_labels=[]\n",
    "    F1_pred=[]\n",
    "    F1_scores=[]\n",
    "    bal_accs=[]\n",
    "\n",
    "    best_network.eval() # preoare model for test and evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #print('Start testing CV...')\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            #print(f'test {i}')\n",
    "\n",
    "            inputs, lables = data\n",
    "\n",
    "            outputs = best_network(inputs)\n",
    "\n",
    "            lables=lables.double()\n",
    "            outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "\n",
    "            #do the prediction \n",
    "            predicted = outputss.data > 0.0  #the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "\n",
    "            #calculate loss\n",
    "            loss = loss_function(outputss, lables)\n",
    "\n",
    "            test_loss_cv += loss.item()\n",
    "            correct += (predicted == lables).sum().item()\n",
    "            total += lables.size(0)\n",
    "\n",
    "            if i==0:\n",
    "                F1_labels=lables.int().numpy()\n",
    "                F1_pred=predicted.int().numpy()\n",
    "            else:\n",
    "                F1_labels= np.concatenate((F1_labels, lables.int().numpy()))\n",
    "                F1_pred = np.concatenate((F1_pred, predicted.int().numpy()))\n",
    "                    \n",
    "    acc_fold = accuracy_score(F1_labels, F1_pred)\n",
    "    bal_acc_fold= balanced_accuracy_score(F1_labels, F1_pred)\n",
    "    F1_Score_fold = f1_score(F1_labels, F1_pred, average='weighted')\n",
    "    \n",
    "    print(f'test_Acc_CV\": {acc_fold}')\n",
    "    print(f'Balanced ACC CV ReHo :{bal_acc_fold}')\n",
    "    print(f'F1_score CV ReHo :{F1_Score_fold}')\n",
    "\n",
    "    \n",
    "    #tn, fp, fn, tp = confusion_matrix(F1_labels, F1_pred).ravel()\n",
    "    \n",
    "    F1_scores.append(F1_Score_fold)\n",
    "    bal_accs.append(bal_acc_fold)\n",
    "        \n",
    "    #PLotting Confusion matrix and ROC curve\n",
    "        \n",
    "    #conf_matrix = confusion_matrix(F1_labels, F1_pred)\n",
    "    #conf_matrix_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "        \n",
    "    #fp_rate, tp_rate, threshold = roc_curve(F1_labels, F1_pred)\n",
    "    #ROC_display = RocCurveDisplay(fpr=fp_rate, tpr=tp_rate).plot()\n",
    "       \n",
    "\n",
    "    #wandb.log({ \"test_balanced_Acc_CV\":bal_acc_fold , \"test_F1_score_CV\": F1_Score_fold,'test_Acc': 100 * correct /total })\n",
    "        \n",
    "\n",
    "F1_score_avg= sum(F1_scores)/len(F1_scores)\n",
    "F1_score_std= statistics.pstdev(F1_scores)\n",
    "bal_acc_avg = 100 * (sum(bal_accs)/len(bal_accs))\n",
    "bal_acc_std = 100* statistics.pstdev(bal_accs)\n",
    "\n",
    "print(f' Average F1_score japan ReHo = { F1_score_avg}')\n",
    "print(f'standard deviation :{F1_score_std}')\n",
    "print(f' Average Balance ACC japan ReHo = {bal_acc_avg}')\n",
    "print(f'standard deviation :{bal_acc_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results of train and validation for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"K-FOLD CROSS VALIDATION RESULTS FOR japanese reho {k_folds} FOLDS\\n--------------------------------\")\n",
    "_sum = 0.0\n",
    "\n",
    "for key, value in kfold_results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    _sum += value\n",
    "\n",
    "print(f'Average: {_sum/len(kfold_results.items())} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop for Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project='mdd-japanese-reho')  \n",
    "\n",
    "#Sample elements randomly \n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset_cv , batch_size=batch_size, shuffle=False)\n",
    "\n",
    "bal_acc_fold=[]\n",
    "F1_score_fold=[]\n",
    "\n",
    "for k in np.arange(5): \n",
    "\n",
    "    print(f'Start TEST FOR FOLD {k}')\n",
    "\n",
    "    path_fold = f'/data/zmohaghegh/TempStats_3D-CNN/best_model_reho_japanese/model-japanese_best-reho-fold-{k}.pth'\n",
    "\n",
    "    network.load_state_dict(torch.load(path_fold))\n",
    "\n",
    "    test_loss_cv=0\n",
    "    total = 0\n",
    "    correct=0\n",
    "\n",
    "    F1_labels=[]\n",
    "    F1_pred=[]\n",
    "\n",
    "    network.eval() # preoare model for test and evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #print('Start testing CV...')\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            #print(f'test {i}')\n",
    "\n",
    "            inputs, lables = data\n",
    "\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            lables=lables.double()\n",
    "            outputss=outputs.squeeze(1) #[10,1] ---> [10]\n",
    "\n",
    "            #do the prediction \n",
    "            predicted = outputss.data > 0.0  #the loss function contain also a sigmoid layer <0 = false >0 =true\n",
    "\n",
    "            #calculate loss\n",
    "            loss = loss_function(outputss, lables)\n",
    "\n",
    "            test_loss_cv += loss.item()\n",
    "            correct += (predicted == lables).sum().item()\n",
    "            total += lables.size(0)\n",
    "\n",
    "            if i==0:\n",
    "                F1_labels=lables.int().numpy()\n",
    "                F1_pred=predicted.int().numpy()\n",
    "            else:\n",
    "                F1_labels= np.concatenate((F1_labels, lables.int().numpy()))\n",
    "                F1_pred = np.concatenate((F1_pred, predicted.int().numpy()))\n",
    "            \n",
    "        #wandb.log({ 'test_Acc': 100 * correct /total  , \"test_Loss\": test_loss_cv/total })\n",
    "        \n",
    "        acc = accuracy_score(F1_labels, F1_pred)\n",
    "        bal_acc= balanced_accuracy_score(F1_labels, F1_pred)\n",
    "\n",
    "        F1_Score = f1_score(F1_labels, F1_pred, average='weighted')\n",
    "        tn, fp, fn, tp = confusion_matrix(F1_labels, F1_pred).ravel()\n",
    "    \n",
    "        F1_score_fold.append(F1_Score)\n",
    "        bal_acc_fold.append(bal_acc)\n",
    "        \n",
    "        #PLotting Confusion matrix and ROC curve\n",
    "        \n",
    "        conf_matrix = confusion_matrix(F1_labels, F1_pred)\n",
    "        conf_matrix_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "        \n",
    "        fp_rate, tp_rate, threshold = roc_curve(F1_labels, F1_pred)\n",
    "        ROC_display = RocCurveDisplay(fpr=fp_rate, tpr=tp_rate).plot()\n",
    "       \n",
    "\n",
    "        wandb.log({ \"test_balanced_Acc_CV\": bal_acc , \"test_F1_score_CV\": F1_Score })\n",
    "        \n",
    "        print(f'test_Acc_CV\": {100 * correct /total}')\n",
    "        print(f'F1_score CV ReHo :{F1_Score}')\n",
    "        print(f'Balanced ACC CV ReHo :{bal_acc}')\n",
    "        print(f'Loss CV ReHo : {test_loss_cv/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_avg= sum(F1_score_fold)/len(F1_score_fold)\n",
    "F1_score_std= statistics.pstdev(F1_score_fold)\n",
    "bal_acc_avg = 100 * (sum(bal_acc_fold)/len(bal_acc_fold))\n",
    "bal_acc_std = 100* statistics.pstdev(bal_acc_fold)\n",
    "\n",
    "print(f' Average F1_score japan ReHo = { F1_score_avg}')\n",
    "print(f'standard deviation :{F1_score_std}')\n",
    "print(f' Average Balance ACC japan ReHo = {bal_acc_avg}')\n",
    "print(f'standard deviation :{bal_acc_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
